# app/core/services/rag_service.py
import logging
from typing import List, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass

from app.core.services.embedding_service import embedding_service
from app.infrastructure.vector_db.vector_repository import VectorRepository
from app.infrastructure.llm.llama2_adapter import Llama2Adapter
from app.config.settings import settings  # Ensure settings are imported
from openai import OpenAI # Import at the top


logger = logging.getLogger(__name__)


@dataclass
class RetrievedCraving:
    """
    Represents a craving retrieved from the vector database.
    """
    id: int
    description: str
    created_at: datetime
    intensity: int
    score: float
    time_score: float = 1.0  # Default time score

class RAGService:
    """
    Service for Retrieval-Augmented Generation (RAG) for craving insights.
    """

    def __init__(self):
        """
        Initialize the RAGService.
        """
        self.vector_repository = VectorRepository()
        self.llm_adapter = Llama2Adapter()  # Or any other LLM adapter
        self.openai_client = OpenAI(api_key=settings.OPENAI_API_KEY)

    def generate_personalized_insight(
        self,
        user_id: int,
        query: str,
        persona: Optional[str] = None,
        top_k: int = 5,
        time_weighted: bool = True
    ) -> str:
        """
        Generate a personalized insight for a user based on a query, using RAG.

        Args:
            user_id: The ID of the user.
            query: The user's query.
            persona: Optional persona for LoRA-based generation.
            top_k: Number of top results to retrieve from the vector database.
            time_weighted: Whether to apply time-based weighting to the results.

        Returns:
            A personalized insight generated by the LLM.
        """
        try:
            # 1. Get embedding for the user's query
            query_embedding = embedding_service.get_embedding(query)

            # 2. Search for relevant cravings
            search_results = self.vector_repository.search_cravings(
                user_id=user_id,
                query_vector=query_embedding,
                limit=top_k * 2  # Fetch more to allow for filtering/weighting
            )

            # 3. Process search results (convert to domain objects, filter, etc.)
            retrieved_cravings = self._process_search_results(search_results)

            # 4. Apply time-based weighting if requested
            if time_weighted:
                retrieved_cravings = self._apply_time_weighting(
                    retrieved_cravings,
                    recency_boost_days=30
                )

            # 5. Sort by score (either base score or time-weighted score)
            retrieved_cravings = sorted(
                retrieved_cravings,
                key=lambda x: x.time_score,
                reverse=True
            )[:top_k]  # Apply top_k after sorting

            # 6. Construct the prompt
            prompt = self._construct_prompt(user_id, query, retrieved_cravings)

            # 7. Generate text with the chosen LLM and persona
            if persona:
                if persona not in settings.LORA_PERSONAS:
                    raise ValueError(f"Persona '{persona}' not configured.")
                answer = self.llm_adapter.generate_text_with_adapter(
                    prompt,
                    persona
                )
            else:
                # Use OpenAI for generation (CORRECTED)
                response = self.openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",  # Or your preferred model
                    messages=[
                        {"role": "system", "content": "You are a helpful AI assistant specializing in craving analysis."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                    max_tokens=500,  # Adjust as needed
                )
                answer = response.choices[0].message.content


            return answer

        except Exception as e:
            logger.error(f"Error in RAG pipeline for user {user_id}: {e}", exc_info=True)
            return (
                "I'm sorry, I'm having trouble generating a personalized insight right now. "
                "Please try again later."
            )
    def _process_search_results(self, search_results) -> List[RetrievedCraving]:
        """Convert raw search results from DB to RetrievedCraving objects."""
        processed_cravings = []
        for result in search_results.get("matches", []):
            try:
                metadata = result['metadata']
                craving = RetrievedCraving(
                    id=int(result['id']),  # Ensure ID is int
                    description=metadata['description'],
                    created_at=datetime.fromisoformat(metadata['created_at']),
                    intensity=int(metadata['intensity']),
                    score=float(result['score'])
                )
                processed_cravings.append(craving)
            except (KeyError, ValueError, TypeError) as e:
                logger.warning(f"Skipping invalid craving result: {result}. Error: {e}")
                continue  # Skip this result and move to the next
        return processed_cravings

    def _apply_time_weighting(
        self,
        cravings: List[RetrievedCraving],
        recency_boost_days: int = 30
    ) -> List[RetrievedCraving]:
        """
        Apply time-based weighting to craving scores.
        
        Args:
            cravings: The list of RetrievedCraving objects.
            recency_boost_days: Cravings within this many days get maximum boost.
            
        Returns:
            The list of cravings with updated time_score.
        """
        now = datetime.utcnow()
        for craving in cravings:
            days_ago = (now - craving.created_at).total_seconds() / (60 * 60 * 24)  # Convert to days
            if days_ago <= recency_boost_days:
                # Linear scaling from 1.0 (today) to ~0.5 (recency_boost_days ago)
                craving.time_score = 1.0 - (days_ago / recency_boost_days) * 0.5
            else:
                # Cravings older than recency_boost_days get a score between ~0.5 and 0.0
                craving.time_score = 0.5 * (1.0 - min(days_ago / 365, 1.0))  # Limit to 1 year for scaling
        return cravings


    def _construct_prompt(
        self,
        user_id: int,
        query: str,
        retrieved_cravings: List[RetrievedCraving]
    ) -> str:
        """Construct the prompt for the LLM."""

        # User profile information
        prompt = "USER PROFILE:\n"
        prompt += f"User ID: {user_id}\n\n"

        # User's query
        prompt += "USER QUERY:\n"
        prompt += f"{query}\n\n"

        # Retrieved craving data
        prompt += "RELEVANT CRAVINGS:\n"
        if retrieved_cravings:
            for craving in retrieved_cravings:
                prompt += f"- ID: {craving.id}\n"
                prompt += f"  Description: {craving.description}\n"
                prompt += f"  Created at: {craving.created_at.strftime('%Y-%m-%d %H:%M:%S')}\n"  # Consistent format
                prompt += f"  Intensity: {craving.intensity}/10\n"
                prompt += f"  Original Score: {craving.score:.3f}\n"
                prompt += f"  Time-weighted Score: {craving.time_score:.3f}\n"
                prompt += "\n"
        else:
            prompt += "No relevant craving data found for this user.\n\n"

        # Instruction for the LLM
        prompt += (
            "Based on the provided user profile and craving history, "
            "please generate a personalized insight to address the user's query. "
            "Consider patterns, triggers, and intensity of cravings. "
            "If no relevant data is available, acknowledge the limitation."
        )

        return prompt


# Instantiate the RAG service (singleton pattern)
rag_service = RAGService()