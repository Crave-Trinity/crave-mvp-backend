# Railway Deployment Guide for CRAVE Backend

This guide explains how to deploy the CRAVE Backend to Railway successfully.

## Prerequisites

- A Railway account
- Access to the CRAVE Backend repository
- Required API keys: OpenAI, Pinecone, HuggingFace

## Deployment Steps

### 1. Create a New Project in Railway

- Go to [Railway](https://railway.app/)
- Click "New Project"
- Select "Deploy from GitHub repo"
- Connect and select the CRAVE Backend repository

### 2. Add a PostgreSQL Database

- In your project, click "New"
- Choose "Database" ‚Üí "PostgreSQL"
- Railway will automatically provision a PostgreSQL database

### 3. Configure Environment Variables

Add the following environment variables to your Railway project:

**Required Variables:**
- `JWT_SECRET` - A secure random string for JWT authentication
- `OPENAI_API_KEY` - Your OpenAI API key
- `PINECONE_API_KEY` - Your Pinecone API key
- `HUGGINGFACE_API_KEY` - Your Hugging Face API key

**Optional Variables:**
- `MIGRATION_MODE` - Set to `upgrade` for the first deployment, then `stamp` for subsequent deployments
- `LLAMA2_MODEL_NAME` - Override the default Llama model if needed
- `PINECONE_ENV` - Override the default Pinecone environment if needed
- `PINECONE_INDEX_NAME` - Override the default Pinecone index name if needed

**Important Note:** 
Do NOT manually set `DATABASE_URL` or `SQLALCHEMY_DATABASE_URI`. Railway automatically provides these for your PostgreSQL instance.

### 4. Deploy Your Application

- Click "Deploy" in your Railway dashboard
- Railway will build and deploy your application using the Dockerfile

### 5. Verify Deployment

- Once deployed, click on the generated domain URL to access your API
- Check the logs for any errors related to database connection or migrations
- Try accessing the `/health` endpoint to verify the API is running

## Troubleshooting Database Connection Issues

If you encounter database connection errors:

1. **Check the logs** for messages about database connection failures
2. **Verify environment variables** in the Railway dashboard
3. **Restart the deployment** to refresh environment variables
4. **Check if your application is trying to use localhost** instead of the Railway database URL

## Update Strategy

When pushing updates to your application:

1. Set `MIGRATION_MODE` to `upgrade` if you've added new database migrations
2. Push your changes to GitHub
3. Railway will automatically detect changes and redeploy your application

## Recommended Railway Settings

- **Autoscaling**: Start with 1 instance and let Railway handle scaling
- **Memory**: 512MB minimum
- **Build Command**: Railway uses your Dockerfile automatically
- **Start Command**: `./entrypoint.sh` (already set in your Dockerfile)

# üåä CRAVE WAVE (Trinity Backend): Vertical AI Optimization for Craving Intelligence

## üåü Overview  
CRAVE WAVE (Trinity Backend) is a modular, Dockerized FastAPI application built with clean architecture principles. It is designed to track and analyze user cravings and integrates multiple external services, including:  

- üõ¢ **PostgreSQL** for structured data storage  
- üß† **Pinecone** for vector-based retrieval 
- ü§ñ **Llama 2 with LoRA integration** for AI-powered insights

I'm actively building an end-to-end system‚Äîfrom initial setup and database migrations to AI model inference with LoRA adapters.  
* üîó [Frontend](https://github.com/Crave-Trinity/crave-trinity-frontend) (SwiftUI + SwiftData + MVVM) 
* üîó [Backend](https://github.com/Crave-Trinity/crave-trinity-backend) (FastAPI + PostgreSQL + Pinecone + Llama 2)
* üîó [Back-End Live](https://crave-trinity-backend-production.up.railway.app/)

‚ö†Ô∏è Disclaimer: CRAVE intends to provide analytical insights based on user-logged cravings data.

It will not offer medical predictions, diagnoses, or treatment prior to FDA SaMD approval.
Any behavioral insights should be viewed as informational only, and users should consult a healthcare professional for medical or therapeutic guidance.

---

![CRAVE Architecture](docs/crave-trinity-architecture-fixed.svg)

## üåä Vision
CRAVE-WAVE
The world's first self-optimizing craving intelligence system‚Äîa backend powered by Vertical AI, ensuring that craving personas, retrieval strategies, and inference evolve dynamically as moat and user behavior shift.

## üöÄ CRAVE WAVE - Finalized Tech Stack  

### **1Ô∏è‚É£ Core Tech Stack**
| **Component**            | **Technology**                                      | **Rationale**  |
|-------------------------|--------------------------------------------------|---------------|
| **LLM Model**           | **Llama 2 (13B) on AWS**                         | Best open-source model that supports LoRA fine-tuning. Not restricted like GPT-4. |
| **Vector Database**      | **Pinecone**                                     | Production-grade, built for high-performance retrieval at scale. |
| **Embeddings**          | **OpenAI `text-embedding-ada-002`**               | Best semantic search embeddings for RAG. |
| **Fine-Tuning Framework** | **LoRA (Low-Rank Adaptation) via PyTorch + Hugging Face `peft`** | Allows persona-level fine-tuning without massive compute costs. |
| **RAG Pipeline**        | **LangChain**                                    | Provides high-level abstractions for orchestrating retrieval, prompt assembly, and response generation. |
| **Backend & Deployment** | **Python (FastAPI) on AWS EC2/ECS**              | Python for ML, FastAPI for async speed, AWS for scalability. |
| **Structured Database**  | **PostgreSQL (AWS RDS)**                        | Stores craving logs, user metadata, and structured behavioral data for analytics & AI modeling. |

<div align="center">
  <img src="assets/images/vertical-ai-orchestrator-fixed.svg" alt="Vertical AI Orchestration System" width="800">
</div>

üß† **Intelligent Persona System**

* AI-powered LoRA hot-swapping ensures only relevant craving personas are active, with dynamic offloading of unused personas and continuous optimization based on real-world triggers
* Reinforcement Learning (RLHF) framework automatically refines persona deployment and selection strategies
* Custom attention mechanisms and adapter weights evolve based on user interaction patterns

üîç **Advanced Retrieval & Memory**

* Time-compressed RAG pipeline that prioritizes recent cravings while intelligently summarizing long-term patterns
* Dynamic retrieval scaling adjusts vector DB query depth and response relevance based on historical interactions
* Intelligent caching system creates compressed trend markers for cost-efficient, rapid retrieval
* Hybrid dense/sparse vector indexing for optimal context matching

‚ö° **Self-Optimizing Infrastructure**

* Real-time VRAM/CPU monitoring with automated resource allocation and scaling
* Quantization and adaptive batching ensure LLaMA/LoRA models maintain peak efficiency
* Distributed inference system automatically manages active nodes to prevent bottlenecks
* Production-grade monitoring stack tracks system health and performance metrics

## üöÄ How It Works ‚Äì End-to-End
### 1Ô∏è‚É£ Craving Data Ingestion
- Apple Watch + iPhone send craving logs (timestamp, HRV, location, user mood, notes).  
- Stored in two places:
  - PostgreSQL (structured metadata like timestamps).  
  - Pinecone (embedded craving logs for retrieval).  

---

### 2Ô∏è‚É£ RAG Personalization ‚Äì How AI Feels Personal Without Full Fine-Tuning 
üîπ **Process:**  
1. User Query: (‚ÄúWhy do I crave sugar at night?‚Äù)  
2. Backend Embeds Query: Uses `text-embedding-ada-002`.  
3. Retrieves Relevant Logs: Pinecone finds most relevant past craving logs.  
4. Compiles Personalized Context: LangChain assembles user history + question into a structured prompt.  
5. LLM Generates a Response: Feeds the retrieved logs + user‚Äôs question to Llama 2.  

‚úÖ Ensures that AI responses feel personalized, without training a separate model per user.  

---

### 3Ô∏è‚É£ LoRA Fine-Tuning ‚Äì Craving Archetypes for Deeper Personalization
üîπ **Why We Need This:**  
- RAG personalizes via past data, but doesn‚Äôt change how the AI "thinks." 
- LoRA lets us create craving-specific personas for better contextualization. 

üîπ **How It Works:**  
1. Users are categorized into craving personas (e.g., ‚ÄúNighttime Binger,‚Äù ‚ÄúStress Craver,‚Äù ‚ÄúAlcohol Dopamine-Seeker‚Äù).  
2. Each persona has a lightweight LoRA adapter fine-tuned on past craving data.  
3. During inference, we dynamically load the relevant LoRA adapter onto Llama 2.  
4. Final Response = RAG Retrieved Context + LoRA Fine-Tuned Persona + User Query.
*  ‚úÖ  This provides "adaptive" AI insights without massive per-user fine-tuning costs.

---

### üöÄ How we make real-time LoRA swapping work efficiently:
‚úÖ Step 1: Load the Base Model into GPU Memory
- Load LLaMA 2 (13B) onto an AWS A100 GPU instance (or H100 if needed).

‚úÖ Step 2: Preload the 2-3 Most Common LoRA Adapters in VRAM
- Track most-used craving personas and keep them loaded in GPU memory.
- Store remaining adapters in CPU RAM for fast retrieval.
  
‚úÖ Step 3: Implement a Fast Cache System for LoRA Adapters
- Store adapters in Redis (or in-memory storage) for quick access.
- If not in VRAM, fetch from CPU RAM before disk.

‚úÖ Step 4: Optimize LoRA Swapping for Concurrency
- Batch requests when multiple users need the same adapter.
- Queue unique adapter loads instead of swapping instantly.
  
‚úÖ Step 5: Monitor GPU Usage & Tune for Performance
Implement profiling to see if we need more VRAM per instance.
If GPU becomes a bottleneck, scale horizontally by adding more instances.

---

### 4Ô∏è‚É£ Data Retention & Time-Based Prioritization
üîπ Problem: As users log cravings for months or years, RAG retrieval becomes bloated.  
üîπ Solution: Implement time-weighted retrieval:  
* ‚úÖ Last 30 Days = High Priority Logs  
* ‚úÖ Older Logs = Summarized & Compressed
* ‚úÖ Historical Insights = Only Retrieved When Highly Relevant 

üîπ **How It Works:**  
- Recent cravings are fully stored & retrieved. 
- Older cravings get "trend compressed" (e.g., "In the last 6 months, sugar cravings spiked in winter").  
- Retrieval automatically prioritizes recent, high-relevance logs. 
- Prevents AI responses from becoming inefficient over time. 

---

## üöÄ Step-by-Step Execution Plan
### ‚úÖ Step 1: Build the Data Pipeline
- Set up FastAPI endpoints for craving logs.  
- Integrate Pinecone to store craving text data.  
- Set up PostgreSQL (or DynamoDB) for structured craving metadata.  

### ‚úÖ Step 2: Implement RAG for Personalized Craving Responses
- Install LangChain + Pinecone for retrieval.  
- Create a retrieval chain that injects user craving logs into AI prompts.  
- Connect the retrieval chain to Llama 2 for personalized AI responses.  

### ‚úÖ Step 3: Build LoRA Fine-Tuned Craving Personas
- Fine-tune Llama 2 LoRA adapters for different craving archetypes using Hugging Face `peft`.  
- Store LoRA adapters separately and **dynamically load them** per user persona.  

### ‚úÖ Step 4: Deploy on AWS & Optimize for Real-Time Inference
- Launch Llama 2 (13B) on an AWS GPU instance (g5.xlarge or A100-based).  
- Set up API endpoints for craving insights.  
- Implement RAG caching & batching for efficiency.  

---

## üöÄ Why This Stack Wins
* ‚úÖ RAG ensures personalization without training individual models.
* ‚úÖ LoRA makes craving personas possible at low cost.
* ‚úÖ AWS GPU hosting means real-time inference at scale.
* ‚úÖ Python + FastAPI = Fast iteration speed & flexibility.
* ‚úÖ The architecture is built to scale, adapt, and improve.

---

## üöÄ Next Steps  
* üí• 1Ô∏è‚É£ Find a visionary technical co-founder
* üí• 2Ô∏è‚É£ Start implementing this backend architecture  
* üí• 3Ô∏è‚É£ Ship, Talk to Users, Iterate
 
---

## üèó Architecture & Batches  

The project was developed with AI-acceleration & basecode abstraction through modular batches, breaking the development process into structured steps:  

### üîπ Batch 1 ‚Äì Initial Setup  
üìå Clone the repository, install dependencies, and configure the environment.  
üîß Initialize PostgreSQL and apply Alembic database migrations.  
üìÇ Key files: `.env`, `requirements.txt`, `alembic.ini`  

### üîπ Batch 2 ‚Äì Backend & Database Integration  
üõ† Develop FastAPI REST endpoints following clean architecture.  
üìä Implement database models, repositories, and use-case layers for craving tracking.  
üìÇ Key files: `app/api/`, `app/core/`, `app/infrastructure/database/`  

### üîπ Batch 3 ‚Äì External Services Integration  
üì° Connect to Pinecone for vector storage & retrieval.  
ü§ñ Integrate OpenAI embeddings for craving analysis.  
üìÇ Key files: `app/infrastructure/vector_db/`, `app/infrastructure/external/openai_embedding.py`  

### üîπ Batch 4 ‚Äì Llama 2 with LoRA Integration  
ü¶ô Load and fine-tune Llama 2 using LoRA adapters.  
üîç Deploy AI inference endpoints for craving insights.  
üìÇ Key files: `app/models/llama2_model.py`, `app/infrastructure/llm/llama2_adapter.py`  

---

## üìÇ File Structure  

```plaintext
jj@DESKTOP-L9V85UA:/mnt/c/Users/JJ/Desktop/CRAVE/crave_trinity_backend$ tree -I ".git"
.
################################################################################
#                                                                              
#  "I understand there's a guy inside me who wants to lay in bed,              
#   smoke weed üçÉ all day, and watch cartoons and old movies.                     
#   My whole life is a series of stratagems to avoid, and outwit, that guy."  
#                                                                              
#   - Anthony Bourdain                                                                                                                         
#                                                                              
################################################################################
#
#
#
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ alembic.ini
‚îú‚îÄ‚îÄ app
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ api
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dependencies.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ endpoints
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ai_endpoints.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ craving_logs.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dependencies.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ health.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ user_queries.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ main.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ config
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __pycache__
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ settings.cpython-310.pyc
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ logging.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ settings.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ container
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ ecs_config.yaml
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ core
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ entities
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ craving.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ user.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ services
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ analytics_service.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ embedding_service.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ lora_manager.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ pattern_detection_service.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ rag_service.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ use_cases
‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ generate_craving_insights.py
‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ ingest_craving.py
‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ manage_metadata.py
‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ process_query.py
‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ search_cravings.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ infrastructure
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ auth
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ auth_service.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ oauth_provider.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ user_manager.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ database
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __pycache__
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ models.cpython-310.pyc
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ migrations
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ README
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __pycache__
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ env.cpython-310.pyc
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ env.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ script.py.mako
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ versions
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 200c7d532370_initial_tables_users_cravings.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ __pycache__
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ 200c7d532370_initial_tables_users_cravings.cpython-310.pyc
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ repository.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ external
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ langchain_integration.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ openai_embedding.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ llm
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ huggingface_integration.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ llama2_adapter.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ lora_adapter.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ vector_db
‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ pinecone_client.py
‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ vector_repository.py
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ models
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ llama2_model.py
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ docs
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ architecture.md
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ roadmap.md
‚îú‚îÄ‚îÄ infra
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ aws
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ docker
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ k8s
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ tests
    ‚îú‚îÄ‚îÄ integration
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ test_ai_endpoints.py
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ test_api.py
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ test_craving_search_api.py
    ‚îú‚îÄ‚îÄ test_basic.py
    ‚îî‚îÄ‚îÄ unit
        ‚îú‚îÄ‚îÄ test_auth_service.py
        ‚îú‚îÄ‚îÄ test_ingest_craving.py
        ‚îú‚îÄ‚îÄ test_lora_adapter.py
        ‚îú‚îÄ‚îÄ test_rag_service.py
        ‚îî‚îÄ‚îÄ test_search_cravings.py

30 directories, 62 files
```
---

### **‚ö° Quick Start**
#### ‚úÖ **Prerequisites**  
Before you begin, ensure you have the following installed:

- üê≥ **Docker & Docker Compose** for containerized setup  
- üêç **Python 3.11** (if running locally)  
- ü§ó **Hugging Face CLI** (for private models & LoRA fine-tuning)  

#### üì• **Clone the Repository**
```bash
git clone https://github.com/Crave-Trinity/crave-trinity-backend.git
cd crave-trinity-backend
```

#### üîß **Configure Environment Variables**  
Create a `.env` file in the project root with the necessary credentials:

```ini
SQLALCHEMY_DATABASE_URI=postgresql://postgres:password@db:5432/crave_db
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_ENV=us-east-1-aws
PINECONE_INDEX_NAME=crave-embeddings
OPENAI_API_KEY=your_openai_api_key_here
```
---

### ü§ó **Set Up Hugging Face Authentication (Required for LoRA & Llama 2)**
1Ô∏è‚É£ **Log in to Hugging Face inside the container:**
```bash
docker exec -it crave_trinity_backend-fast-api-1 bash
huggingface-cli login
```
When prompted, **paste your Hugging Face access token** (get it from https://huggingface.co/settings/tokens).

2Ô∏è‚É£ **Enable Git credential storage (to avoid re-authenticating):**
```bash
git config --global credential.helper store
```

3Ô∏è‚É£ **Verify authentication:**
```bash
huggingface-cli whoami
```
‚úÖ **If you see your username and "Token valid (permission: write)," you're good to go!** üöÄ

---

### üèó **Build & Run with Docker Compose**
```bash
docker-compose up --build
```
This will:  
‚úÖ Build the FastAPI backend container  
‚úÖ Start the PostgreSQL database  
‚úÖ Expose ports 8000 (API) & 5432 (Database)  

---

### üîÑ **Run Database Migrations**
Inside the container (or locally, if configured):  
```bash
alembic upgrade head
```
This ensures the database schema is up to date.

---

### üß™ **Testing the Application**
#### üî¨ **API Endpoints**
Once running, test the craving logging API:
```bash
curl -X POST -H "Content-Type: application/json" \
-d '{"user_id":1, "description":"Chocolate craving", "intensity":8}' \
http://localhost:8000/cravings
```

#### üì° **Pinecone Integration**
Inside the FastAPI container, verify the Pinecone index:
```bash
docker exec -it crave_trinity_backend-fast-api-1 python -c \
"from app.infrastructure.vector_db.pinecone_client import init_pinecone; \
init_pinecone(); import pinecone; print('List of indexes:', pinecone.list_indexes())"
```
‚úÖ Ensure `crave-embeddings` exists and is ready for use.

---

### ü§ñ **Run Llama 2 with LoRA Inference (Batch 4)**
```bash
docker exec -it crave_trinity_backend-fast-api-1 python app/models/llama2_model.py
```
‚úÖ This loads **Llama 2 + LoRA adapters** and runs a test inference prompt.

---

## üõ† Technical Details  

- üê≥ Dockerized Setup  
  - The backend is containerized with Python 3.11-slim for efficiency.  

- üõ¢ Database
  - Uses PostgreSQL, managed via Alembic migrations.  

- üì° External Services 
  - Pinecone for vector storage & retrieval.  
  - OpenAI for text embeddings and craving analysis.  

- ü§ñ AI Model (Batch 4) 
  - Llama 2 runs via Hugging Face Transformers.  
  - LoRA adapters fine-tune AI insights with PEFT.  

---

## üõ£ Roadmap & Future Enhancements  

üîú Batch 5 ‚Äì Analytics dashboard & craving trend visualization  
üìä Batch 6 ‚Äì Performance optimizations (GPU inference, rate limiting)  
üîí Security Enhancements ‚Äì OAuth, data anonymization, and logging improvements  
üöÄ Scaling ‚Äì Kubernetes deployment (`infra/k8s`)  

---

üåç Why This Changes Everything

* üí• Static AI is dead.
* üí• Self-learning, self-optimizing AI is the future.
* üí• CRAVE-WAVE is that future.

üí° We don‚Äôt ask if something is possible. We build until it is.

* ‚ö° AI that doesn‚Äôt just process cravings‚Äîit evolves in real time.
* ‚ö° Welcome to the first self-optimizing craving intelligence system.
* ‚ö° Welcome to CRAVE-WAVE.

üî• Get Involved & Contribute
This is a revolution in craving intelligence.

üìú GitHub: Crave-Trinity Backend
üì¢ Twitter: Coming Soon.
üéô YC Demo Day: Stay tuned.

---

* üîó Frontend (SwiftUI + SwiftData + MVVM) ‚Üí [crave-trinity-frontend](https://github.com/Crave-Trinity/crave-trinity-frontend)
* üîó Backend (FastAPI + PostgreSQL + Pinecone + Llama 2) ‚Üí [crave-trinity-backend](https://github.com/Crave-Trinity/crave-trinity-backend)

---

‚ö†Ô∏è Disclaimer: CRAVE intends to provide analytical insights based on user-logged cravings data.

It will not offer medical predictions, diagnoses, or treatment prior to FDA SaMD approval.
Any behavioral insights should be viewed as informational only, and users should consult a healthcare professional for medical or therapeutic guidance.

---

## ü§ù Contributing  

1Ô∏è‚É£ **Fork** the repository  
2Ô∏è‚É£ **Create** a feature branch (`git checkout -b feature/your-feature`)  
3Ô∏è‚É£ **Commit** your changes (`git commit -m "Added feature X"`)  
4Ô∏è‚É£ **Push** & open a pull request  

---

## üìú License  

This project is licensed under the **MIT License**.  
> CRAVE: Because understanding your cravings shouldn't be complicated üç´
